<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Engineering: Binning, Scaling, and Transformation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            overflow-x: hidden;
        }

        .presentation-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .slide {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            opacity: 0;
            transform: translateY(30px);
            animation: slideIn 0.8s ease-out forwards;
        }

        .slide:nth-child(even) {
            animation-delay: 0.2s;
        }

        .slide:nth-child(odd) {
            animation-delay: 0.1s;
        }

        @keyframes slideIn {
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .slide:hover {
            transform: translateY(-5px);
            transition: transform 0.3s ease;
            box-shadow: 0 25px 50px rgba(0, 0, 0, 0.15);
        }

        h1 {
            font-size: 3.5em;
            text-align: center;
            margin-bottom: 20px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
        }

        h2 {
            font-size: 2.5em;
            margin-bottom: 25px;
            color: #2c3e50;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            position: relative;
        }

        h2::after {
            content: '';
            position: absolute;
            bottom: -3px;
            left: 0;
            width: 0;
            height: 3px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            animation: expandLine 2s ease-out forwards;
        }

        @keyframes expandLine {
            to {
                width: 100%;
            }
        }

        h3 {
            font-size: 1.8em;
            margin: 25px 0 15px 0;
            color: #34495e;
            position: relative;
            padding-left: 20px;
        }

        h3::before {
            content: '‚ñ∂';
            position: absolute;
            left: 0;
            color: #667eea;
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        p, li {
            font-size: 1.1em;
            line-height: 1.8;
            margin-bottom: 15px;
            text-align: justify;
        }

        ul {
            padding-left: 25px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
            position: relative;
        }

        li::marker {
            color: #667eea;
            font-weight: bold;
        }

        .code-block {
            background: linear-gradient(135deg, #2c3e50, #34495e);
            color: #ecf0f1;
            padding: 20px;
            border-radius: 15px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            overflow-x: auto;
            border-left: 5px solid #667eea;
            box-shadow: inset 0 0 20px rgba(0, 0, 0, 0.2);
        }

        .highlight {
            background: linear-gradient(120deg, rgba(102, 126, 234, 0.2), rgba(118, 75, 162, 0.2));
            padding: 15px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
            margin: 20px 0;
            font-style: italic;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .comparison-table th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 20px;
            text-align: left;
            font-size: 1.2em;
            font-weight: bold;
        }

        .comparison-table td {
            padding: 15px 20px;
            border-bottom: 1px solid #e0e6ed;
            transition: background-color 0.3s ease;
        }

        .comparison-table tr:hover td {
            background-color: rgba(102, 126, 234, 0.05);
        }

        .example-box {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1), rgba(118, 75, 162, 0.1));
            border: 2px solid rgba(102, 126, 234, 0.3);
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            position: relative;
            overflow: hidden;
        }

        .example-box::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: linear-gradient(45deg, transparent, rgba(255, 255, 255, 0.1), transparent);
            transform: rotate(45deg);
            animation: shine 3s infinite;
        }

        @keyframes shine {
            0% { transform: translateX(-100%) translateY(-100%) rotate(45deg); }
            100% { transform: translateX(100%) translateY(100%) rotate(45deg); }
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 25px;
            margin: 25px 0;
        }

        .pros, .cons {
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
        }

        .pros {
            background: linear-gradient(135deg, rgba(46, 204, 113, 0.1), rgba(39, 174, 96, 0.1));
            border-left: 5px solid #2ecc71;
        }

        .cons {
            background: linear-gradient(135deg, rgba(231, 76, 60, 0.1), rgba(192, 57, 43, 0.1));
            border-left: 5px solid #e74c3c;
        }

        .formula {
            background: #f8f9fa;
            border: 2px dashed #667eea;
            border-radius: 10px;
            padding: 20px;
            text-align: center;
            font-family: 'Times New Roman', serif;
            font-size: 1.3em;
            margin: 20px 0;
            box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.05);
        }

        .navigation {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
        }

        .nav-btn {
            background: rgba(102, 126, 234, 0.9);
            color: white;
            border: none;
            padding: 12px 20px;
            margin: 5px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 0.9em;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
        }

        .nav-btn:hover {
            background: rgba(118, 75, 162, 0.9);
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }

        @media (max-width: 768px) {
            .slide {
                padding: 25px;
                margin-bottom: 20px;
            }
            
            h1 { font-size: 2.5em; }
            h2 { font-size: 2em; }
            h3 { font-size: 1.5em; }
            
            .pros-cons {
                grid-template-columns: 1fr;
            }
            
            .navigation {
                position: relative;
                text-align: center;
                margin: 20px 0;
            }
        }
    </style>
</head>
<body>
    <div class="navigation">
        <button class="nav-btn" onclick="scrollToSection('title')">Top</button>
        <button class="nav-btn" onclick="scrollToSection('binning')">Binning</button>
        <button class="nav-btn" onclick="scrollToSection('scaling')">Scaling</button>
        <button class="nav-btn" onclick="scrollToSection('transformation')">Transformation</button>
    </div>

    <div class="presentation-container">
        <!-- Title Slide -->
        <div class="slide" id="title">
            <h1>Feature Engineering</h1>
            <h2 style="text-align: center; border: none; font-size: 2em; margin-top: 30px;">Binning, Scaling, and Transformation</h2>
            <div class="highlight">
                <p style="text-align: center; font-size: 1.3em; margin-bottom: 0;">
                    Transforming raw data into meaningful features for machine learning success
                </p>
            </div>
        </div>

        <!-- Introduction -->
        <div class="slide">
            <h2>What is Feature Engineering?</h2>
            <p>Feature engineering is the process of using domain knowledge to extract features from raw data that make machine learning algorithms work more effectively. It's often considered the most crucial step in the machine learning pipeline, as the quality of features directly impacts model performance.</p>
            
            <h3>Why Feature Engineering Matters</h3>
            <ul>
                <li>Improves model accuracy and performance</li>
                <li>Reduces training time and computational complexity</li>
                <li>Helps algorithms converge faster</li>
                <li>Makes patterns in data more apparent</li>
                <li>Reduces overfitting by creating meaningful representations</li>
            </ul>

            <div class="highlight">
                <strong>Key Insight:</strong> "Garbage in, garbage out" - even the most sophisticated algorithms cannot compensate for poorly engineered features.
            </div>
        </div>

        <!-- Binning Section -->
        <div class="slide" id="binning">
            <h2>Binning (Discretization)</h2>
            <p>Binning is the process of converting continuous numerical variables into discrete categorical variables by grouping values into bins or intervals. This technique helps capture non-linear relationships and reduces the impact of outliers.</p>

            <h3>Types of Binning</h3>
            
            <div class="example-box">
                <h4><strong>1. Equal-Width Binning</strong></h4>
                <p>Divides the range of values into intervals of equal size.</p>
                <div class="formula">
                    Bin Width = (Max Value - Min Value) / Number of Bins
                </div>
                <div class="code-block">
# Python Example
import pandas as pd
import numpy as np

# Create sample data
ages = [18, 25, 35, 45, 55, 65, 75, 85]

# Equal-width binning
bins = pd.cut(ages, bins=4, labels=['Young', 'Adult', 'Middle-aged', 'Senior'])
print(bins)</div>
            </div>

            <div class="example-box">
                <h4><strong>2. Equal-Frequency Binning (Quantile Binning)</strong></h4>
                <p>Divides data so that each bin contains approximately the same number of observations.</p>
                <div class="code-block">
# Equal-frequency binning
bins = pd.qcut(ages, q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
print(bins)</div>
            </div>

            <div class="example-box">
                <h4><strong>3. Custom Binning</strong></h4>
                <p>Uses domain knowledge to create meaningful intervals.</p>
                <div class="code-block">
# Custom binning based on domain knowledge
custom_bins = [0, 18, 35, 60, 100]
labels = ['Child', 'Young Adult', 'Adult', 'Senior']
bins = pd.cut(ages, bins=custom_bins, labels=labels)
print(bins)</div>
            </div>

            <div class="pros-cons">
                <div class="pros">
                    <h4><strong>‚úÖ Advantages</strong></h4>
                    <ul>
                        <li>Reduces impact of outliers</li>
                        <li>Captures non-linear relationships</li>
                        <li>Simplifies complex distributions</li>
                        <li>Makes data more interpretable</li>
                        <li>Reduces overfitting</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4><strong>‚ùå Disadvantages</strong></h4>
                    <ul>
                        <li>Loss of information and granularity</li>
                        <li>Arbitrary bin boundaries</li>
                        <li>May create artificial patterns</li>
                        <li>Requires domain knowledge for optimal binning</li>
                        <li>Can introduce bias</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Scaling Section -->
        <div class="slide" id="scaling">
            <h2>Feature Scaling</h2>
            <p>Feature scaling is the process of normalizing the range of independent variables or features of data. It ensures that all features contribute equally to the distance calculations in algorithms that are sensitive to the scale of input features.</p>

            <h3>Common Scaling Techniques</h3>

            <div class="example-box">
                <h4><strong>1. Min-Max Scaling (Normalization)</strong></h4>
                <p>Scales features to a fixed range, typically [0, 1].</p>
                <div class="formula">
                    X_scaled = (X - X_min) / (X_max - X_min)
                </div>
                <div class="code-block">
from sklearn.preprocessing import MinMaxScaler

# Example data
data = [[1, 2], [2, 4], [3, 6], [4, 8]]

# Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)</div>
            </div>

            <div class="example-box">
                <h4><strong>2. Standard Scaling (Z-score Normalization)</strong></h4>
                <p>Centers data around mean (0) with unit variance (1).</p>
                <div class="formula">
                    X_scaled = (X - Œº) / œÉ
                </div>
                <div class="code-block">
from sklearn.preprocessing import StandardScaler

# Standard scaling
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)</div>
            </div>

            <div class="example-box">
                <h4><strong>3. Robust Scaling</strong></h4>
                <p>Uses median and interquartile range, less sensitive to outliers.</p>
                <div class="formula">
                    X_scaled = (X - median) / IQR
                </div>
                <div class="code-block">
from sklearn.preprocessing import RobustScaler

# Robust scaling
scaler = RobustScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)</div>
            </div>

            <table class="comparison-table">
                <tr>
                    <th>Scaling Method</th>
                    <th>Range</th>
                    <th>Best Used When</th>
                    <th>Outlier Sensitivity</th>
                </tr>
                <tr>
                    <td>Min-Max</td>
                    <td>[0, 1]</td>
                    <td>Known min/max bounds</td>
                    <td>High</td>
                </tr>
                <tr>
                    <td>Standard</td>
                    <td>Mean=0, Std=1</td>
                    <td>Normal distribution</td>
                    <td>High</td>
                </tr>
                <tr>
                    <td>Robust</td>
                    <td>Variable</td>
                    <td>Data has outliers</td>
                    <td>Low</td>
                </tr>
            </table>
        </div>

        <!-- Transformation Section -->
        <div class="slide" id="transformation">
            <h2>Feature Transformation</h2>
            <p>Feature transformation involves applying mathematical functions to features to change their distribution, reduce skewness, or make relationships more linear. This helps improve model performance and interpretability.</p>

            <h3>Common Transformation Techniques</h3>

            <div class="example-box">
                <h4><strong>1. Log Transformation</strong></h4>
                <p>Reduces right skewness and compresses large values.</p>
                <div class="formula">
                    X_transformed = log(X + 1)
                </div>
                <div class="code-block">
import numpy as np

# Log transformation
data = [1, 10, 100, 1000, 10000]
log_transformed = np.log1p(data)  # log(1 + x)
print(log_transformed)</div>
            </div>

            <div class="example-box">
                <h4><strong>2. Square Root Transformation</strong></h4>
                <p>Mild transformation for moderately skewed data.</p>
                <div class="formula">
                    X_transformed = ‚àöX
                </div>
                <div class="code-block">
# Square root transformation
sqrt_transformed = np.sqrt(data)
print(sqrt_transformed)</div>
            </div>

            <div class="example-box">
                <h4><strong>3. Box-Cox Transformation</strong></h4>
                <p>Automatically finds the best power transformation.</p>
                <div class="formula">
                    X_transformed = (X^Œª - 1) / Œª  (if Œª ‚â† 0)<br>
                    X_transformed = log(X)  (if Œª = 0)
                </div>
                <div class="code-block">
from scipy.stats import boxcox

# Box-Cox transformation
data_positive = [1, 2, 3, 4, 5, 10, 20, 50]
transformed, lambda_value = boxcox(data_positive)
print(f"Lambda: {lambda_value}")
print(f"Transformed: {transformed}")</div>
            </div>

            <div class="example-box">
                <h4><strong>4. Polynomial Features</strong></h4>
                <p>Creates polynomial and interaction features.</p>
                <div class="code-block">
from sklearn.preprocessing import PolynomialFeatures

# Create polynomial features
X = [[1, 2], [3, 4], [5, 6]]
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
print(X_poly)
# Output includes: x1, x2, x1^2, x1*x2, x2^2</div>
            </div>

            <div class="pros-cons">
                <div class="pros">
                    <h4><strong>‚úÖ When to Transform</strong></h4>
                    <ul>
                        <li>Highly skewed distributions</li>
                        <li>Non-linear relationships</li>
                        <li>Wide range of values</li>
                        <li>Need for feature interactions</li>
                        <li>Improving model assumptions</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4><strong>‚ùå Considerations</strong></h4>
                    <ul>
                        <li>Loss of interpretability</li>
                        <li>Increased complexity</li>
                        <li>Potential overfitting</li>
                        <li>Computational overhead</li>
                        <li>May not always improve performance</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Best Practices -->
        <div class="slide">
            <h2>Best Practices & Guidelines</h2>
            
            <h3>Feature Engineering Workflow</h3>
            <div class="example-box">
                <ol>
                    <li><strong>Understand Your Data:</strong> Explore distributions, correlations, and patterns</li>
                    <li><strong>Handle Missing Values:</strong> Impute or remove missing data appropriately</li>
                    <li><strong>Detect Outliers:</strong> Identify and decide how to handle extreme values</li>
                    <li><strong>Apply Transformations:</strong> Based on data distribution and model requirements</li>
                    <li><strong>Scale Features:</strong> Ensure all features contribute equally</li>
                    <li><strong>Create New Features:</strong> Use domain knowledge to derive meaningful features</li>
                    <li><strong>Validate Results:</strong> Test feature engineering impact on model performance</li>
                </ol>
            </div>

            <h3>Algorithm-Specific Considerations</h3>
            <table class="comparison-table">
                <tr>
                    <th>Algorithm Type</th>
                    <th>Scaling Needed?</th>
                    <th>Transformation Benefits</th>
                </tr>
                <tr>
                    <td>Linear Models</td>
                    <td>Yes (Standard/Min-Max)</td>
                    <td>High - improves convergence</td>
                </tr>
                <tr>
                    <td>Tree-based</td>
                    <td>No</td>
                    <td>Low - naturally handle non-linearity</td>
                </tr>
                <tr>
                    <td>Neural Networks</td>
                    <td>Yes (Standard preferred)</td>
                    <td>High - improves training stability</td>
                </tr>
                <tr>
                    <td>Distance-based (KNN, SVM)</td>
                    <td>Yes (Essential)</td>
                    <td>Medium - depends on data distribution</td>
                </tr>
            </table>

            <div class="highlight">
                <strong>Remember:</strong> Always fit scalers and transformers on training data only, then apply to validation and test sets to prevent data leakage!
            </div>
        </div>

        <!-- Practical Example -->
        <div class="slide">
            <h2>Comprehensive Example</h2>
            <p>Let's walk through a complete feature engineering pipeline using a sample dataset:</p>

            <div class="code-block">
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split

# Sample dataset
data = {
    'age': [25, 35, 45, 55, 65, 75, 85, 95],
    'income': [30000, 50000, 80000, 120000, 90000, 60000, 40000, 25000],
    'education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'High School', 'High School', 'Bachelor']
}

df = pd.DataFrame(data)

# 1. Binning age into categories
df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 70, 100], 
                        labels=['Young', 'Adult', 'Middle-aged', 'Senior'])

# 2. Log transformation for income (reduce skewness)
df['income_log'] = np.log1p(df['income'])

# 3. Encoding categorical variables
le = LabelEncoder()
df['education_encoded'] = le.fit_transform(df['education'])

# 4. Scaling numerical features
scaler = StandardScaler()
numerical_features = ['age', 'income_log']
df[['age_scaled', 'income_scaled']] = scaler.fit_transform(df[numerical_features])

# 5. Creating polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['age_scaled', 'income_scaled']])

print("Original Data:")
print(df.head())
print("\nEngineered Features Shape:", poly_features.shape)
</div>

            <div class="highlight">
                <strong>Result:</strong> We've transformed our simple 3-feature dataset into a rich feature set that captures non-linear relationships, handles categorical data, and normalizes scales - all ready for machine learning!
            </div>
        </div>

        <!-- Conclusion -->
        <div class="slide">
            <h2>Key Takeaways</h2>
            
            <div class="example-box">
                <h3>üéØ Essential Points to Remember</h3>
                <ul>
                    <li><strong>Data Understanding First:</strong> Always explore your data before applying transformations</li>
                    <li><strong>Algorithm Matters:</strong> Choose techniques based on your model requirements</li>
                    <li><strong>Avoid Data Leakage:</strong> Fit transformations on training data only</li>
                    <li><strong>Validate Impact:</strong> Measure how feature engineering affects model performance</li>
                    <li><strong>Iterate and Experiment:</strong> Feature engineering is an iterative process</li>
                </ul>
            </div>

            <div class="highlight">
                <strong>Success Formula:</strong> Good feature engineering = Better models + Faster training + More interpretable results
            </div>

            <h3>Next Steps</h3>
            <p>Continue exploring advanced feature engineering techniques like feature selection, dimensionality reduction (PCA, t-SNE), and domain-specific feature creation. Remember, the art of feature engineering often distinguishes good data scientists from great ones!</p>
        </div>
    </div>

    <script>
        function scrollToSection(sectionId) {
            const element = document.getElementById(sectionId);
            if (element) {
                element.scrollIntoView({ 
                    behavior: 'smooth',
                    block: 'start'
                });
            }
        }

        // Add smooth scrolling animation on load
        window.addEventListener('load', function() {
            const slides = document.querySelectorAll('.slide');
            slides.forEach((slide, index) => {
                slide.style.animationDelay = `${index * 0.1}s`;
            });
        });

        // Add parallax effect to background
        window.addEventListener('scroll', function() {
            const scrolled = window.pageYOffset;
            const rate = scrolled * -0.5;
            document.body.style.backgroundPosition = `center ${rate}px`;
        });
    </script>
</body>
</html>